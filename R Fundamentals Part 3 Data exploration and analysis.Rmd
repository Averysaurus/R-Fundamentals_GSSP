---
title: 'R Fundamentals Part 3: Data exploration and analysis'
author: "D-Lab"
date: "September 12, 2017"
output:
  word_document:
    toc: yes
  html_document:
    toc: yes
    toc_float: yes
---
 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Part 2 review
1. Loading data from files
2. One-dimensional subsetting
3. Two-dimensional subsetting: column names, positive integers, negative integers, logical indices.  
4. Manually defining factor variables.  

Set your working directory to the R-Fundamentals data folder
```{r, eval=FALSE}
getwd()
setwd("/Users/evan.muzzall/Desktop/R-Fundamentals/data")
```

Read in the `animals` data frame you created on Day 1
```{r}
animals <- read.csv("/Users/evan.muzzall/Desktop/R-Fundamentals/data/animals.csv", header=TRUE, stringsAsFactors=FALSE)
head(animals)
```

Coerce `Type` to factor type. See (this page)[https://www.stat.berkeley.edu/classes/s133/factors.html] for help with factors. 
```{r}
str(animals)
animals$Type <- factor(animals$Type)
str(animals)
```

Part 3 Learning objectives
1. Day 2 review
2. Data exploration - summarization and plotting
3. What is hypothesis testing?
4. Fast intro to statistical testing
5. ggplot2

# 1.  Data exploration and analysis in R
Research design, data collection, exploration, visualization, and analysis are fundamental aspects of research. Learning how to explore and analyze data will help you "think with the data" so that you become able to formulate better research designs and more efficient data collection protocols for your own research.

Furthermore, when you read peer-reviewed work in your field, having an understanding of basic summaries, plots, and statistical tests will help you better grasp arguments that the authors are making and will allow you to more critically evaluate their rationale. 

Our focus is on data analysis in R, which usually proceeds in two steps:
1) exploration/summarizing/graphing
2) statistical testing  

Data exploration often means summarizing and graphing your data to see if it is possible to visualize any obvious (or not so obvious) trends. 

Statistical testing is employed to test relationships that you might have noticed in the exploration stage. Remember that we are not formally teaching you statistics, rather responsible statistical programming application in R.  

> NOTE: you want to use data exploration and testing to make some sort of informed decision about the question you are asking!  

# 2.  Data summarization and plotting  
This section will cover some quick ways to summarize and plot data.  

### 2.1 Data summarization (`summary()`)
It is often useful to first summarize your data. In R, this can be done in a variety of ways. 

`summary()` provides a six-number summary of a data frame:
```{r}
summary(animals)

# or of a single vector

summary(animals$Height)
```

### 2.2 Data summarization (`describe()` and `describeBy()`)
`describe()` and `describeBy()` from the `psych` R package provide more in-depth summary information. However, we are going to subset the data so that it only includes the numeric variables within the `describe()` call. 

Remember, we installed the 'psych' package on Day 1, so all we have to do is call it into our environment with `library`:
```{r}
library(psych)

describe(animals) # this causes errors

str(animals)
describe(animals[,c(3:5)]) # this looks cleaner
```
We can also subset these summary statistics to create simpler tables. Let's save this output first: 
```{r}
animals_describe <- describe(animals[,c(3:5)])
animals_describe
```

Now we can subset it like we would a regular data frame.  

If we just want "mean", "median", and "sd", we could subset our data frame like this:
```{r}
animals_simple <- animals_describe[,c(3, 5, 4)]
animals_simple
```

This is convenient to quickly write output to file! 
```{r, eval=FALSE}
write.csv(animals_simple, "animals_simple.csv", row.names=TRUE)
#`row.names=TRUE` ensures that row names "Weight", "Height", and "Progress" labels are printed.
```
Check your working directory for the new .CSV file!  

How else can you check your working directory?  

We can also describe our data by a grouping variable using `describeBy()`. What if we want summary statistics not for the whole sample like above, but for each animal type (i.e., Cat, Dog, and Pig?)
```{r, eval=FALSE}
#Output summary statistics by one grouping variable:
summary_sub <- describeBy(animals[,c(3:5)], animals$Type)
summary_sub

#If we just want to view Cats, Dogs, or Pigs, we can type:
summary_sub$Cat
summary_sub$Dog
summary_sub$Pig

#We can also view just the means for Dogs, we can type:
summary_sub$Dog[[3]] #Here, double bracket subsetting is used to return just the value from a list.

#If we just want the Height mean for Dogs (8.5), we can type:
summary_sub$Dog[[3]][2]

#Or the medians for Pigs:
summary_sub$Pig[[5]]
summary_sub$Pig[["median"]]
```

### 2.3 Data summarization (`table()`)
We can view frequencies for of categorical data like `animals$Type` with `table()`
```{r}
table(animals$Type) # get frequencies for Cat, Dog, and Pig

table(animals$Type, animals$Healthy) #get frequencies for Cat, Dog, and Pig by column "Healthy".
```

Wow! Only 2/7 cats are healthy.  
3/6 dogs are healthy.  
6/7 pigs are healthy.   

Thus, seemingly simple tabulations can help us make informe decisions. Why are cats the least healthy? Perhaps now we can devote resources to the care of cats.  

# 3.  Plotting
Visualizing data is useful for exploration. R offers a variety of base plots and a multitude of additional packages for plotting.  

### 3.1 Plotting (`hist()`)  
Histograms are a handy way to quickly illustrate the distribution shape of a particular variable. Let's visualize the "Height" variable from the `animals` data frame:
```{r}
hist(animals$Weight, col="violetred")
```

The `col=` argument specifies the color of the bars. Look at the colors you can choose from in base R using `colors()`
```{r, eval=FALSE}
colors() 
```

Some basic arguments you will want to make note of:  
-`main` - change plot title  
-`xlab` - change x-axis label  
-`ylab` - change y-axis label  
-`las` - change orientation of the tick mark text  
```{r}
hist(animals$Weight, 
     col = "violetred",
     main = "Histogram of animal weight",
     xlab = "Animal weight",
     ylab = "FREQUENCY",
     las=1)
```

#**Challenge 1**
1. Use `summary()`, `table()` and `describeBy()` to investigate variables in the `iris` dataset. Create a histogram of one of the numeric variables. 

### 3.1 Plotting (`plot()`)  
What if we want to know more about the relationships between two numeric variables? Here, base R's `plot()` function is useful. Use this to create a scatterplot.

Remember earlier that we coerced "Type" from character to factor data type? This helps us map colors and shapes to specific factor levels: 
```{r}
str(animals)
```

Then, identify two numeric/integer variables you want to plot on the x and y axes:
```{r, eval=FALSE}
animals$Weight
animals$Height
```
Now, we can plot animal "Weight" versus "Height":
```{r}
plot(x = animals$Weight, y = animals$Height,    
    xlab="Weight (kg)",   # change x axis label
    ylab="Height (cm)",   # change y axis label
    main="Animal weights and heights",   # add plot title
    las=1, # make all axis text parallel to x-axis
    col=as.integer(animals$Type),   # change point colors to correspond to animal types
    pch=as.integer(animals$Type),   # change point symbols to correspond to animal types
    cex=2)   # change point size 
```

> There is extensive help available for finding help with colors in R. See (here)[https://www.stat.ubc.ca/~jenny/STAT545A/block14_colors.html] and (here)[https://www.stat.ubc.ca/~jenny/STAT545A/block15_colorMappingBase.html].  

> Much help also exists for changing point shapes in (base R)[http://www.sthda.com/english/wiki/r-plot-pch-symbols-the-different-point-shapes-available-in-r] and (ggplot2)[http://www.sthda.com/english/wiki/ggplot2-point-shapes]. 

However, in our examples the points are falling ourside the plot boundaries! We can change that with `xlim` and `ylim`:
```{r}
plot(animals$Weight, animals$Height,    
    xlab="Weight (kg)",   # change x axis label
    ylab="Height (cm)",   # change y axis label
    main="Animal weights and heights",   # add plot title
    las=1, # make all axis text parallel to x-axis
    col=as.integer(animals$Type),   # change point colors to match animal types
    pch=as.integer(animals$Type),   # change point symbols to match animal types
    cex=2,   # change point size 
    xlim=c(3,8),  # adjust range of x axis
    ylim=c(4,11))  # adjust range of y axis

# That looks better! We can also add a legend:

legend("topright", inset=.0, title="Animal", cex=1,
       c("Cat","Dog","Pig"), col=c(1,2,3), pch=c(1,2,3),
       horiz=FALSE)
```

> NOTE: One drawback of R Studio is that its in-environment graphics look a little strange sometimes. However, to save it as a beautiful .PNG file, we wrap our plotting and legend instructions by `png()` and `dev.off()` lines of code. We can specify figure characteristics inside `png()` such as the dimensions and resolution of the image, while `dev.off()` writes the file to disk.

> Alternatively, you can simply click the "export" button in your plot GUI window pane.  

Highlight and run all of this code at once. This will create a 6 x 6 inch figure at 300 dpi.  
```{r, eval=FALSE}
png("Animal weights and heights.png", height=6, width=6, units="in", res=300)
plot(animals$Weight, animals$Height,    
    xlab="Weight (kg)", ylab="Height (cm)", 
    main="Animal weights and heights", las=1,
    col=as.integer(animals$Type), pch=as.integer(animals$Type), 
    cex=2, xlim=c(3,8), ylim=c(3,11))
legend("topright", inset=.0, title="Animal", cex=1,
       c("Cat","Dog","Pig"), col=c(1,2,3), pch=c(1,2,3),
       horiz=FALSE)
dev.off()
```
Voil√†! Check your working directory - we now have a publishable quality figure that can be copy and pasted into your manuscript :)  

#**Challenge 2**
1. Using the `iris` dataset, plot two numeric variables and export the graph to your working directory as a .PNG file. 

### 3.3 Plotting (`boxplot()`)  
Boxplots also are useful because you can graphically represent a numeric variable by levels of a factor. The idea is similar to `hist()` and `plot()` except the syntax is different. In R, these are often referred to as (Tukey boxplots)[http://mathworld.wolfram.com/Box-and-WhiskerPlot.html].    
```{r}
boxplot(animals$Height ~ animals$Type)
```
The tilde `~` means "as parsed by" or "as divided by" some category (in our case Height as parsed _by_ Cat, Dog, and Pig). It means we want to look at animal Height by animal Type.  

Remember your six number summary from earlier? A Tukey boxplot represents a 5 number summary (these six numbers minus the mean).  

The "whiskers" of these box and whisker plots represent the minimum and maximum values.  

The lower and upper borders of the rectangle represent the first and third quartiles.  

The thick horizontal black bar inside the box represents the median!  

We can also change colors by specifying a vector of color names with a length of 3 (one for each level of our factor). Other base plotting features apply as well:  
```{r}
boxplot(animals$Height ~ animals$Type,
        col=c("aquamarine", "goldenrod", "salmon"),
        main="Animal boxplots", 
        xlab="Type", ylab="Height", 
        las=1)
#That looks better! 
```
We can also save our figures as .PDF files:
```{r, eval=FALSE}
pdf("animals boxplot.pdf", 6, 6)
boxplot(animals$Height ~ animals$Type,
        col=c("aquamarine", "goldenrod", "salmon"),
        main="Animal boxplots", 
        xlab="Type", ylab="Height", 
        las=1)
dev.off()
```
Check your working directory again - you now have a .PDF of your boxplots! 

#**Challenge 3**
1. Create boxplots for one of the numeric variables in the `iris` dataset and export it to your working directory as a .PDF.  

# 4.  Statistical testing
Now that we have sufficiently summarized and explored our data, it is time to formally investigate some of the potential relationships identified by it. 

> NOTE: remember that we are _not_ formally teaching you statistics, but instead how statistical programming works in R. View the "course offerings" section of the readme file in the (R-Fundamentals repository)[https://github.com/dlab-berkeley/R-Fundamentals] to learn how to learn statistics here at UC Berkeley!  

### 4.1 Hypothesis testing
You will frequently encounter hypotheses in the peer-reviewed literature and in the output of test results in R. Hypotheses generally should be based on research questions and expectations about the data. 

**Hypothesis testing** is central to research. How can you tell if the differences you observe are statistically real or not? We answer the question through hypothesis formulation and significance testing as measured by p-values.  

The simplest way to think about hypothesis testing is that you have two hypotheses: the null (Ho) and the alternative (Ha). 

The null hypothesis can be thought of as **NO RELATIONSHIP** or **NO DIFFERENCE** .

The alternative hypothesis can be thought of as **SOME SORT OF RELATIONSHIP** or **SOME SORT OF DIFFERENCE**.  

Should your test fail to achieve statistical significance you **FAIL TO REJECT THE NULL HYPOTHESIS** and by default accept the conditions of the null hypothesis. 

Should your test achieve statistical significance, you **REJECT THE NULL HYPOTHESIS** and by default, accept the terms of the alternative hypothesis.  
### 4.2 P-value
In this oversimplified example these are your only two options and are tested using a "p-value". A p-value signifies how likely differences are statistically real instead of due to random chance. The most common standard cutoff value is p<0.05.  

If p>0.05, you **FAIL TO REJECT THE NULL HYPOTHESIS**! This means that the null hypothesis holds true and that there is "no difference" between your groups or "no relationship" among the variables being tested.  

If p<0.05, you **REJECT THE NULL HYPOTHESIS** of "no difference" and by default you accept the alternative hypothesis and whatever "differences" or "relationships" it specified. This is a _"statistically significant"_ difference.  

For example, look at your boxplots. Cat and Pig medians appear similar for Height, but their standard deviations differ greatly (the position of the median relative to the distribution shape of the boxes and whiskers). However, they both seem different from Dog...  

Might there be formal significant statistical differences between the means of Cat/Pig and Dog?  

Well, we can test those assumptions with "mean comparisons" such as t-tests and analysis of variance (ANOVA)!  

### 4.3 Statistical testing (`t.test()`)  
A "t.test" formally compares _one or two group means_ for statistically significant differences at a standard p-value cutoff of p<0.05. We can use `t.test` to make an observation of a population based on a sample. We are only examining a few hypothetical cats, dogs, and pigs - if we had ALL cats, dogs, and pigs in the world, there would be no need for a statistic - instead, we would only be concerned with what we observe!   

> KEY POINT: a t-test should only be applied for comparisons between one or two groups! Use an ANOVA (discussed below) for _more than two groups!_  

The null hypothesis states that there are no actual mean differences between the two groups. 

#**Challenge 4**
Subsetting review! 
1. Subset your "animals" data frame into three new dataframes: Cat, Dog, and Pig. "Cat" should contain only cats, "Dog" should contain only dogs, and "Pig" should contain only pigs. 
```{r}
Cat <- animals[animals$Type == "Cat",]
Dog <- animals[animals$Type == "Dog",]
Pig <- animals[animals$Type == "Pig",]
```

Now, we can compare Height means between two of the three animals (we will use an ANOVA to compare all three group means while responsibly correcting for family wise error - see below). 
```{r}
t.test(Cat$Height, Pig$Height)
# The p-value is 0.88 - we FAIL TO REJECT the null hypothesis (aka, there is no statistical difference between Cat and Pig means for Height). We accept the conditions of the null hypothesis that state there are no between-group differences. 
```
What about between Cat and Dog?
```{r}
t.test(Cat$Height, Dog$Height)
# p=0.39 so again we FAIL TO REJECT the null hypothesis
```

# **Challenge 5**
1. Subset the `iris` dataset by species and perform a t-test between two of the species on one of the numeric variables.  

### 4.4 Statistical testing (`aov()` and `TukeyHSD()`)
What about if we want to compare more than two groups? Doing multiple pairwise t-tests for **1)** cat v. dog, **2)** cat v. pig, **3)** dog v. pig) is not ideal because it becomes more difficult to adjust for the "family wise error rate", or the influence of the other groups/variables that are present but not necessarily being being tested.  

When we want to compare _more than two group means at once_ (i.e., Cat, Dog, _and_ Pig), we can use an analysis of variance (ANOVA). This is called by `aov()` in R. 

We can follow `aov()` with a "Tukey test of Honest Significant Difference" to find out between exactly which groups the differences (if any) exist. This is called with `TukeyHSD` in R.

Let's try! Note the slightly different syntax - we now specify only the column headings, and enter the name of our data frame in the "data" argument. Let's call our object `aov1`, which is an object we can unpack and look inside:
```{r}
aov1 <- aov(Height ~ Type, data = animals)

#Use `summary()` to access the useful information from our `aov1` model:
summary(aov1)
```
We receive a non-significant p-value. However, we can still follow this up with a TukeyHSD test to see between which between-group differences do contribute to differences although they are non-significant: 
```{r}
TukeyHSD(aov1)
#This allows us to see mean Height differences bewteen the multiple comparisons animals, with adjusted p-values. 
```

# **Challenge 6**
1. Using the `iris` dataset, perform an ANOVA and TukeyHSD test for one of the numeric variables between the three flower species. In general, what are differences between t-tests and ANOVAs that you should keep in mind?  

### 4.4 Statistical testing (`cor.test()`)  
Correlation is a useful way to see if two numeric variables are related. "Pearson's r" is the default coefficient in `cor.test()`.  

Investigate if Cat Height and Cat Weight are correlated:
```{r}
cor.test(Cat$Height, Cat$Weight)
```
Almost! Cat Height and Weight are NEGATIVELY correlated (r = -0.69) and are almost statistically significant (p = 0.08). This is a good example of having to deal with p-values that approach statistical significance in real research.  

What do you do with a p-value of 0.08?  

### 4.5 Statistical testing (`lm()`)  
So, Cat Height and Weight seem to be _almost_ significantly related at p<0.05.  

Now what? Linear regression is a convenient way to see if one numeric variable can be used to predict another. Do you think Cat Weight can be used to predict Cat Height? Again, note the altered syntax:

`model1 <- lm(Y ~ X, data=data)`

Y is the target, response, outcome, or **_dependent_** variable. This is the variable we want to predict.  

X is the predictor or **_independent_** variable.  

We can use X (Weight) to predict the outcome of Y (Heigiht) using our animals data set!  
```{r}
lin_model1 <- lm(Height ~ Weight, data = animals)
summary(lin_model1)
```
`lm()` output is dense! Check out [the yhat blog for fitting and interpreting linear models](http://blog.yhat.com/posts/r-lm-summary.html). Also be aware that we have only minimally discussed data assumptions for using `lm()`. 

> DISCLAIMER! If you are interested in any of these tests presented in this lesson, make sure you consult the prerequisite data assumptions for these tests (i.e., normality, trait independence, etc.), your advisor, and/or a statistician.  

You can also pull items out of return objects using `names()`. To extract the residuals we would use the dollar sign operator `$` and type:
```{r, eval=FALSE}
names(lin_model1)
lin_model1$residuals
```

# **Challenge 7**
1. Using the `mtcars` dataset, create boxplots for one numeric variable as parsed by a factor variable.  
2. How do you load the "mtcars" dataset?  
3. How do you view the variable names of this dataset?  
4. Might you surmise a relationship about something like engine size and miles per gallon?  
5. What does `cor.test` reveal about engine size and miles per gallon?  
6. Create a scatterplot of two variables using `plot()`.  
7. Can one of these variables be used to predict another in a linear regression model?  

# 5.  ggplot2
The "ggplot2" R package is a powerful way to graph your data. The syntax is similar to what is used in "dplyr", except instead of a pipe `%>%`, a plus-symbol `+` is used:
```{r}
library(ggplot2)
?ggplot2
mtcars$cyl <- factor(mtcars$cyl) # first, coerce "cyl" to factor data type
ggplot(mtcars, aes(x=cyl, y=mpg, fill=cyl)) + 
  geom_boxplot() + 
  theme_bw()
```

Note the different syntax again. You need three things to make a ggplot:  
1. A dataset (`mtcars`)  
2. aesthetics `aes()` - this is where you specify your axes and colors  
3. A geom - this is where you tell R if you want a scatterplot, boxplots, etc.  

Let's create a scatterplot using the `iris` data:
```{r}
data(iris)
ggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = Species)) + 
  geom_point() + 
  #theme_light() + 
  #theme_dark() + 
  #theme_classic() + 
  #theme_bw + 
  theme(legend.position="top") + 
  theme(axis.text.x=element_text(angle=45, hjust=1)) + 
  theme(axis.text.y=element_text(angle=45, hjust=1))
```

> REMEMBER: If you have research questions, [schedule an appointment with a D-Lab consultant!](http://dlab.berkeley.edu/consulting)

Acknowledgements:
- Contributions by Evan Muzzall, Rochelle Terman, Dillon Niederhut. 